#!/usr/bin/env python3

"""
Authors: Andrew Blair, Ivo Violich
python merge_metadata.py --flist s3.files --type HiFi
"""
import sys
import os
import re
import argparse
import textwrap
import pandas as pd

# Setup argument parser
parser = argparse.ArgumentParser(
    formatter_class=argparse.RawDescriptionHelpFormatter,
    description=textwrap.dedent('''\
        Process HPRC metadata files based on S3 file list.

        Expects a submissions directory with subdirectories:
            1_submitter_metadata containing .tsv files
            5_readstats with either the output of mergeSRAMetaReadstats.ipynb or a Readstats table, in .tsv
            8_sra_metadata containing the 'metadata processes ok' files generated by SRA

        Input is the result of: 
        aws s3 ls s3://human-pangenomics/working --recursive --profile <your_profile> > s3.files
        aws s3 ls s3://human-pangenomics/working --recursive --no-sign-request > s3.files
    ''')
)

# Required arguments
group = parser.add_argument_group('required arguments')
group.add_argument('--flist', type=str, help='s3 file list')
group.add_argument('--prepend', type=str, default='s3://human-pangenomics/', help='URL prefix for S3 files')
group.add_argument('--type', type=str, help='Wildcard for which submissions to include: HiFi, ONT, DEEPCONSENSUS')

# Exit if no arguments are provided
if len(sys.argv) == 1:
    parser.print_help()
    sys.exit(1)

args = parser.parse_args()

# Function to list submission directories
def list_submissions(parent_directory='submissions', type_wildcard=''):
    """
    List first-level submission directories, excluding 'empty_submission'.
    """
    subdirectories = []
    for item in os.listdir(parent_directory):
        item_path = os.path.join(parent_directory, item)
        if type_wildcard in item and os.path.isdir(item_path) and 'empty_submission' not in item_path:
            subdirectories.append(item_path)
    return subdirectories

# Function to find .tsv files in specific subdirectories
def find_tsv_files(submission_dirs, subdir):
    """
    Expects .tsv files in specific subdirectories of the submissions.
    """
    files = []
    for dirpath in submission_dirs:
        subdirpath = os.path.join(dirpath, subdir)
        if os.path.exists(subdirpath):
            for item in os.listdir(subdirpath):
                if item.endswith('.tsv'):
                    files.append(os.path.join(subdirpath, item))
    return files

# Function to combine .tsv files, keeping only common columns
def combine_files(tsvfiles):
    """
    Keep only the columns that are present in all files, excluding 'file_size', 'md5sum', and 'filetype'.
    """
    common_columns = None
    df_combined = pd.DataFrame()

    for file_path in tsvfiles:
        temp_df = pd.read_csv(file_path, sep='\t')
        if common_columns is None:
            common_columns = set(temp_df.columns)
            df_combined = temp_df.copy()
        else:
            common_columns &= set(temp_df.columns)
            df_combined = pd.concat([df_combined, temp_df[list(common_columns)]], ignore_index=True, sort=False)

    drop_columns = ['file_size', 'md5sum', 'filetype']
    columns_present = all(col in df_combined.columns for col in drop_columns)

    if columns_present:
        # Drop columns
        return df_combined.drop(columns=['file_size', 'md5sum', 'filetype'])
    else:
        return df_combined

# Function to combine readstats from .tsv files
def combine_readstats(tsvfiles, data_type):
    """
    Combine readstats from .tsv files, keeping only relevant columns based on the data type.
    """
    # Define columns to keep for each data type
    keep_columns = {
        "ONT": ['filename', 'read_N50', 'Gb', 'coverage', '100kb+', '200kb+', '300kb+', '400kb+', '500kb+', '1Mb+', 'whales'],
        "DEEPCONSENSUS": ['filename', 'total_reads', 'total_bp', 'total_Gbp', 'min', 'max', 'mean', 
                          'quartile_25', 'quartile_50', 'quartile_75', 'N25', 'N50', 'N75'],
        "HiFi": ['filename', 'total_reads', 'total_bp', 'total_Gbp', 'min', 'max', 'mean', 
                 'quartile_25', 'quartile_50', 'quartile_75', 'N25', 'N50', 'N75']
    }
    
    if data_type not in keep_columns:
        raise ValueError(f"Unsupported data type: {data_type}")

    df_readstats = pd.DataFrame()
    for file_path in tsvfiles:
        temp_df = pd.read_csv(file_path, sep='\t', usecols=keep_columns[data_type])
        df_readstats = pd.concat([df_readstats, temp_df], ignore_index=True)
    
    return df_readstats

# Function to combine SRA metadata
def combine_sra(tsvfiles):
    """
    Combine SRA metadata files, keeping only accession IDs and related columns.
    """
    keep = ['accession', 'study', 'biosample_accession']
    sra_df_combined = pd.DataFrame()
    
    for file_path in tsvfiles:
        temp_df = pd.read_csv(file_path, delimiter='\t')
        temp_df.rename(columns={'filename': 'filename_original'}, inplace=True)
        # Handle multiple filename fields
        filename_columns = [col for col in temp_df.columns if col.startswith('filenam')]
        temp_df_melted = pd.melt(temp_df, id_vars=keep, value_vars=filename_columns, value_name='filename').drop('variable', axis=1)
        temp_df_filtered = temp_df_melted.dropna(subset=['filename'])
        sra_df_combined = pd.concat([sra_df_combined, temp_df_filtered], ignore_index=True, sort=False)
    
    return sra_df_combined

# Function to list files from S3 based on file type
def bucket_files(flist, prepend, file_type):
    """
    Get all HPRC and HPRC_PLUS files of the given type from the S3 input file list.
    """
    rows = []
    file_extension = {
        "HiFi": "bam",
        "ONT": "bam", # Update fastq.gz to bam
        "DEEPCONSENSUS": "fastq.gz"
    }.get(file_type)

    if not file_extension:
        raise ValueError(f"Unsupported file type: {file_type}")

    with open(flist, 'r') as bucket:
        for line in bucket:
            line = line.strip()
            if line.endswith(file_extension) and '/HPRC' in line:
                fpath = line.split()[-1]
                rows.append({'filename': os.path.basename(fpath), 'path': prepend + fpath})

    return pd.DataFrame(rows, columns=['filename', 'path'])

# Function to merge dataframes by 'filename' column
def merge_by_filename(dflist):
    """
    Merge DataFrames by the 'filename' column.
    """
    merged_df = dflist[0]
    for df in dflist[1:]:
        merged_df = pd.merge(merged_df, df, on='filename')
    return merged_df

# def wrangle_HiFi(submission_dirs)


# Main execution block
def main():

    # TODO: 
    # Add assert check with submission dirs length for submitter_metadata, readstats, and sra_metadata
    # Add unit test to Git Repo

    # NOTE: 
    # 04.08.24
    # Hold merging the HiFi TopUp directory (incomplete- new file more coverage with new machine Revio)
    # HiFI metadata addition: css algorithm version

    # MVP on DEEPCONSENSUS_v1pt2_2023_08_q20
    # Do not include the HPRC_DEEPCONSENSUS_v1pt2_2023_08_q20_readstats.tsv
    # Include deepconsensus algorithm version

    # MVP on ONT Y2 
    # Include the NTSM file with the NTSM score and result (convert 1_submitter_metadata from txt to tsv) 
    # Missing sra metadata because the bam files were too large
    # Missing file_size and md5sum

    #TODO: 
    # Reverse check meta data for every file in working
    # 

    submission_dirs = list_submissions(type_wildcard=args.type)
    # print('Modality: ' + args.type)
    # print('\n')

    if args.type == 'HiFi':
        submission_dirs = [folder for folder in submission_dirs if folder.split('/')[-1] != 'WUSTL_HPRC_HiFi_Year3_TopUp']


    if args.type == 'DEEPCONSENSUS':
        submission_dirs = [folder for folder in submission_dirs if folder.split('/')[-1] == 'HPRC_DEEPCONSENSUS_v1pt2_2023_08_q20']
    
    # print('# submission_dirs')
    # for submission_file in submission_dirs:
    #     print(submission_file)
    # print(len(submission_dirs))
    # print('\n')

    # Process metadata files from different submission directories
    # print('# 1_submitter_metadata')
    submitter_files = find_tsv_files(submission_dirs, '1_submitter_metadata')
    if args.type == 'DEEPCONSENSUS':
        submitter_files = [file for file in submitter_files if file.split('/')[-1].endswith('_submitter_metadata.tsv')]

    # for submitter_file in submitter_files:
    #     print(submitter_file)
    # print(len(submitter_files))
    # print('\n')

    # print('# 5_readstats')
    readstat_files = find_tsv_files(submission_dirs, '5_readstats')
    if args.type == 'DEEPCONSENSUS':
        readstat_files = [file for file in readstat_files if file.endswith('_post_sra_metadata.tsv')]    
    # for readstat_file in readstat_files:
    #     print(readstat_file)
    # print(len(readstat_files))
    # print('\n')

    # print('# 8_sra_metadata')
    sra_files = find_tsv_files(submission_dirs, '8_sra_metadata')
    # for sra_file in sra_files:
    #     print(sra_file)
    # print(len(sra_files))
    # print('\n')

    # Check sample files are all present
    if args.type == 'ONT':
        pass
    else:
        assert len(submission_dirs) == len(submitter_files) == len(readstat_files) == len(sra_files)

    ### Merge ###
    # 1_submitter_metadata
    submitter_df = combine_files(submitter_files) 

    if args.type == 'DEEPCONSENSUS':
        required_deepconsensus_submitter_columns = ['filename', 'sample_ID', 'library_ID', 'library_strategy', 'library_source', 'library_selection', 'library_layout', 'platform', 'instrument_model', 'design_description', 'data_type', 'shear_method', 'size_selection', 'DeepConsensus_version', 'polymerase_version', 'seq_plate_chemistry_version', 'generator_facility', 'generator_contact', 'notes']
        assert list(submitter_df) == required_deepconsensus_submitter_columns
    
    if args.type == 'HiFi':
        required_hifi_submitter_columns = ['filename', 'sample_ID', 'library_ID', 'library_strategy', 'library_source', 'library_selection', 'library_layout', 'platform', 'instrument_model', 'design_description', 'data_type', 'shear_method', 'size_selection', 'ccs_algorithm', 'polymerase_version', 'seq_plate_chemistry_version', 'generator_facility', 'generator_contact', 'notes']
        assert list(submitter_df) == required_hifi_submitter_columns

    if args.type == 'ONT':
        submitter_df = submitter_df.drop(columns=['Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26']) # NOTE: 4.05.24 Agree to manual drop unnamed nan columns
        required_ONT_submitter_columns = ['filename', 'filetype', 'sample_ID', 'library_ID', 'library_strategy', 'library_source', 'library_selection', 'library_layout', 'platform', 'instrument_model', 'design_description', 'data_type', 'shear_method', 'size_selection', 'seq_kit', 'basecaller', 'basecaller_version', 'basecaller_model', 'generator_facility', 'generator_contact', 'notes']
        assert list(submitter_df) == required_ONT_submitter_columns

    # 5_readstats
    if args.type == 'ONT': # split by NTSM and summary files
        df_ntsm_readstats = pd.DataFrame()
        df_readstats = pd.DataFrame()
        for file_path in readstat_files:
            if file_path.endswith('_NTSM.tsv'):
                # TODO: Integrate this into combine_readstats
                temp_df = pd.read_csv(file_path, sep='\t', usecols=['sample','ntsm_score','result', 'ONT_pass_bam'])
                df_ntsm_readstats = pd.concat([df_ntsm_readstats, temp_df], ignore_index=True)
            else:
                temp_df = pd.read_csv(file_path, sep='\t', usecols=['File', 'read_N50', 'Gb', 'coverage', '100kb+', '200kb+', '300kb+', '400kb+', '500kb+', '1Mb+', 'whales'])
                df_readstats = pd.concat([df_readstats, temp_df], ignore_index=True)
        df_ntsm_readstats['ONT_pass_bam'] = [file.split('/')[-1] for file in df_ntsm_readstats['ONT_pass_bam'].tolist()]

        df_ntsm_readstats.columns = ['sample', 'filename', 'ntsm_score', 'result'] # Change ONT_pass_bam to filename
        df_readstats.columns = ['filename', 'read_N50', 'Gb', 'coverage', '100kb+', '200kb+', '300kb+', '400kb+', '500kb+', '1Mb+', 'whales']

        df_readstats['filename'] = [s.replace(" ", "") for s in df_readstats['filename'].tolist()]
        df_ntsm_readstats['filename'] = [s.replace(" ", "") for s in df_ntsm_readstats['filename'].tolist()]
        readstats_df = pd.merge(df_readstats, df_ntsm_readstats, on='filename', how='inner')

    else:
        readstats_df = combine_readstats(readstat_files, args.type)
        assert list(readstats_df) == ['filename', 'total_reads', 'total_bp', 'total_Gbp', 'min', 'max', 'mean', 'quartile_25', 'quartile_50', 'quartile_75', 'N25', 'N50', 'N75']

    # 8_sra_metadata
    if args.type == 'ONT':
        pass
    else:
        sra_df = combine_sra(sra_files)
        assert list(sra_df) == ['accession', 'study', 'biosample_accession', 'filename']

    # Process S3 file list and merge data
    bucket_df = bucket_files(args.flist, args.prepend, args.type)
    bucket_df.to_csv('s3-bucket.tsv', sep='\t')
    if args.type == 'ONT':
        merged_df = merge_by_filename([bucket_df, readstats_df, submitter_df])
    else:
        merged_df = merge_by_filename([bucket_df, sra_df, readstats_df, submitter_df])

        # Check if all SRA submissions are present in the merged dataframe
        if sra_df.shape[0] > merged_df.shape[0]:
            print("Warning: Not all SRA submissions are present in the merged dataframe.", file=sys.stderr)

    # Sort and reorder columns, with 'sample_ID' as second column
    merged_df.sort_values(by=['filename'], inplace=True)
    cols = list(merged_df.columns)
    cols.insert(1, cols.pop(cols.index('sample_ID')))
    merged_df = merged_df[cols]

    # Write output to a TSV file
    output_filename = f"hprc_metadata_sample_files_{args.type}.tsv"
    merged_df.to_csv('data-tables/sample-files/'+output_filename, sep='\t', index=False)
    print(f"Merged metadata written to {output_filename}")

if __name__ == "__main__":
    main()
