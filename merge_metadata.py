#!/usr/bin/env python3

import sys
import os
import re
import argparse
import textwrap
import pandas as pd

# Setup argument parser
parser = argparse.ArgumentParser(
    formatter_class=argparse.RawDescriptionHelpFormatter,
    description=textwrap.dedent('''\
        Process HPRC metadata files based on S3 file list.

        Expects a submissions directory with subdirectories:
            1_submitter_metadata containing .tsv files
            5_readstats with either the output of mergeSRAMetaReadstats.ipynb or a Readstats table, in .tsv
            8_sra_metadata containing the 'metadata processes ok' files generated by SRA

        Input is the result of: 
        aws s3 ls s3://human-pangenomics/working --recursive --profile <your_profile> > s3.files
    ''')
)

# Required arguments
group = parser.add_argument_group('required arguments')
group.add_argument('--flist', type=str, help='s3 file list')
group.add_argument('--prepend', type=str, default='s3://human-pangenomics/', help='URL prefix for S3 files')
group.add_argument('--type', type=str, help='Wildcard for which submissions to include: HiFi, ONT, DEEPCONSENSUS')

# Exit if no arguments are provided
if len(sys.argv) == 1:
    parser.print_help()
    sys.exit(1)

args = parser.parse_args()

# Function to list submission directories
def list_submissions(parent_directory='submissions', type_wildcard=''):
    """
    List first-level submission directories, excluding 'empty_submission'.
    """
    subdirectories = []
    for item in os.listdir(parent_directory):
        item_path = os.path.join(parent_directory, item)
        if type_wildcard in item and os.path.isdir(item_path) and 'empty_submission' not in item_path:
            subdirectories.append(item_path)
    return subdirectories

# Function to find .tsv files in specific subdirectories
def find_tsv_files(submission_dirs, subdir):
    """
    Expects .tsv files in specific subdirectories of the submissions.
    """
    files = []
    for dirpath in submission_dirs:
        subdirpath = os.path.join(dirpath, subdir)
        if os.path.exists(subdirpath):
            for item in os.listdir(subdirpath):
                if item.endswith('.tsv'):
                    files.append(os.path.join(subdirpath, item))
    return files

# Function to combine .tsv files, keeping only common columns
def combine_files(tsvfiles):
    """
    Keep only the columns that are present in all files, excluding 'file_size', 'md5sum', and 'filetype'.
    """
    common_columns = None
    df_combined = pd.DataFrame()

    for file_path in tsvfiles:
        temp_df = pd.read_csv(file_path, sep='\t')
        if common_columns is None:
            common_columns = set(temp_df.columns)
            df_combined = temp_df.copy()
        else:
            common_columns &= set(temp_df.columns)
            df_combined = pd.concat([df_combined, temp_df[list(common_columns)]], ignore_index=True, sort=False)

    return df_combined.drop(columns=['file_size', 'md5sum', 'filetype'])

# Function to combine readstats from .tsv files
def combine_readstats(tsvfiles, data_type):
    """
    Combine readstats from .tsv files, keeping only relevant columns based on the data type.
    """
    # Define columns to keep for each data type
    keep_columns = {
        "ONT": ['filename', 'read_N50', 'Gb', 'coverage', '100kb+', '200kb+', '300kb+', '400kb+', '500kb+', '1Mb+', 'whales'],
        "DEEPCONSENSUS": ['filename', 'total_reads', 'total_bp', 'total_Gbp', 'min', 'max', 'mean', 
                          'quartile_25', 'quartile_50', 'quartile_75', 'N25', 'N50', 'N75'],
        "HiFi": ['filename', 'total_reads', 'total_bp', 'total_Gbp', 'min', 'max', 'mean', 
                 'quartile_25', 'quartile_50', 'quartile_75', 'N25', 'N50', 'N75']
    }
    
    if data_type not in keep_columns:
        raise ValueError(f"Unsupported data type: {data_type}")

    df_readstats = pd.DataFrame()
    for file_path in tsvfiles:
        temp_df = pd.read_csv(file_path, sep='\t', usecols=keep_columns[data_type])
        df_readstats = pd.concat([df_readstats, temp_df], ignore_index=True)
    
    return df_readstats

# Function to combine SRA metadata
def combine_sra(tsvfiles):
    """
    Combine SRA metadata files, keeping only accession IDs and related columns.
    """
    keep = ['accession', 'study', 'biosample_accession']
    sra_df_combined = pd.DataFrame()
    
    for file_path in tsvfiles:
        temp_df = pd.read_csv(file_path, delimiter='\t')
        temp_df.rename(columns={'filename': 'filename_original'}, inplace=True)
        # Handle multiple filename fields
        filename_columns = [col for col in temp_df.columns if col.startswith('filenam')]
        temp_df_melted = pd.melt(temp_df, id_vars=keep, value_vars=filename_columns, value_name='filename').drop('variable', axis=1)
        temp_df_filtered = temp_df_melted.dropna(subset=['filename'])
        sra_df_combined = pd.concat([sra_df_combined, temp_df_filtered], ignore_index=True, sort=False)
    
    return sra_df_combined

# Function to list files from S3 based on file type
def bucket_files(flist, prepend, file_type):
    """
    Get all HPRC and HPRC_PLUS files of the given type from the S3 input file list.
    """
    rows = []
    file_extension = {
        "HiFi": "bam",
        "ONT": "fastq.gz",
        "DEEPCONSENSUS": "fastq.gz"
    }.get(file_type)

    if not file_extension:
        raise ValueError(f"Unsupported file type: {file_type}")

    with open(flist, 'r') as bucket:
        for line in bucket:
            line = line.strip()
            if line.endswith(file_extension) and '/HPRC' in line:
                fpath = line.split()[-1]
                rows.append({'filename': os.path.basename(fpath), 'path': prepend + fpath})

    return pd.DataFrame(rows, columns=['filename', 'path'])

# Function to merge dataframes by 'filename' column
def merge_by_filename(dflist):
    """
    Merge DataFrames by the 'filename' column.
    """
    merged_df = dflist[0]
    for df in dflist[1:]:
        merged_df = pd.merge(merged_df, df, on='filename')
    return merged_df

# Main execution block
def main():
    submission_dirs = list_submissions(type_wildcard=args.type)

    # Process metadata files from different submission directories
    submitter_files = find_tsv_files(submission_dirs, '1_submitter_metadata')
    readstat_files = find_tsv_files(submission_dirs, '5_readstats')
    sra_files = find_tsv_files(submission_dirs, '8_sra_metadata')

    submitter_df = combine_files(submitter_files)
    readstats_df = combine_readstats(readstat_files, args.type)
    sra_df = combine_sra(sra_files)

    # Process S3 file list and merge data
    bucket_df = bucket_files(args.flist, args.prepend, args.type)
    merged_df = merge_by_filename([bucket_df, sra_df, readstats_df, submitter_df])

    # Check if all SRA submissions are present in the merged dataframe
    if sra_df.shape[0] > merged_df.shape[0]:
        print("Warning: Not all SRA submissions are present in the merged dataframe.", file=sys.stderr)

    # Sort and reorder columns, with 'sample_ID' as second column
    merged_df.sort_values(by=['filename'], inplace=True)
    cols = list(merged_df.columns)
    cols.insert(1, cols.pop(cols.index('sample_ID')))
    merged_df = merged_df[cols]

    # Write output to a TSV file
    output_filename = f"hprc_metadata_{args.type}.tsv"
    merged_df.to_csv(output_filename, sep='\t', index=False)
    print(f"Merged metadata written to {output_filename}")

if __name__ == "__main__":
    main()
